{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preparing the dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfhQyjUUTZVV+U7CrLIO6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshana-22/Music-Genre-Classification/blob/main/Implementing%20the%20neural%20network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsWr2bshKkMe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQL4Q524QQ-U",
        "outputId": "72a4f391-e6aa-40b0-b68d-c13345cf7c7c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7TjS_cuQNL0"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrS9g5gwh0dz"
      },
      "source": [
        "DATASET_PATH = \"/content/genres_original\"\n",
        "JSON_PATH = \"data.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30 # in seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxo4c0ZdYdfv"
      },
      "source": [
        "def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
        "  # dictionary to store data\n",
        "  data={\n",
        "      \"mapping\": [],\n",
        "      \"mfcc\":[],\n",
        "      \"labels\": []\n",
        "  }\n",
        "  num_samples_per_segment = int (SAMPLES_PER_TRACK / num_segments)\n",
        "  expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment/ hop_length)\n",
        "\n",
        "  for i, (dirpath, dirnames, filenames) in enumerate (os.walk(dataset_path)):\n",
        "    if dirpath is not dataset_path:\n",
        "      # save the semantic label\n",
        "      dirpath_components = dirpath.split(\"/\")\n",
        "      semantic_label = dirpath_components[-1]\n",
        "      data[\"mapping\"].append(semantic_label)\n",
        "      print(\"\\nprocessing {}\".format(semantic_label))\n",
        "\n",
        "      for f in filenames:\n",
        "        #loading audio file\n",
        "        file_path = os.path.join(dirpath, f)\n",
        "        signal, sr = librosa.load(file_path, sr= SAMPLE_RATE)\n",
        "\n",
        "        # process segments extracting mfcc and storing data\n",
        "        for s in range(num_segments):\n",
        "          start_sample = num_samples_per_segment * s\n",
        "          finish_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "\n",
        "          mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n",
        "                                      sr = sr,\n",
        "                                      n_fft = n_fft,\n",
        "                                      n_mfcc = n_mfcc ,\n",
        "                                      hop_length = hop_length)\n",
        "          \n",
        "          \n",
        "          mfcc = mfcc.T\n",
        "          if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
        "            data[\"mfcc\"].append(mfcc.tolist())\n",
        "            data[\"labels\"].append(i-1)\n",
        "            print(\"{}, segment:{}\".format(file_path, s+1))\n",
        "\n",
        "  with open(json_path, \"w\") as fp:\n",
        "     json.dump(data, fp, indent=4)   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPaL9vflv865"
      },
      "source": [
        "save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0PxMW6LA_V7"
      },
      "source": [
        "DATASET_PATH = \"/content/data.json\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYCKjBPO_nCS"
      },
      "source": [
        "def load_data(dataset_path):\n",
        "  with open(dataset_path, \"r\") as fp:\n",
        "    data= json.load(fp)\n",
        "\n",
        "    #convert lists into numpy arrays\n",
        "    inputs = np.array(data[\"mfcc\"])\n",
        "    targets = np.array(data[\"labels\"])\n",
        "    print(\"Data successfully loaded!!\")\n",
        "\n",
        "    return inputs, targets"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO6tUoewA3Br",
        "outputId": "2fa1af24-2806-4b35-a971-033c21c8424e"
      },
      "source": [
        "inputs, targets = load_data(DATASET_PATH)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data successfully loaded!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nxdd-llA0r1"
      },
      "source": [
        "#split the data into train and test set\n",
        "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs,targets, test_size= 0.3)\n",
        "\n",
        "#build the model\n",
        "model = keras.Sequential([\n",
        "                          #input layer\n",
        "                          keras.layers.Flatten(input_shape=(inputs.shape[1], inputs.shape[2])),\n",
        "\n",
        "                          #1st hidden layer\n",
        "                          keras.layers.Dense(512, activation=\"relu\"),\n",
        "\n",
        "                          #2nd hidden layer\n",
        "                          keras.layers.Dense(256, activation=\"relu\"),\n",
        "\n",
        "                          #3rd hidden layer\n",
        "                          keras.layers.Dense(64, activation=\"relu\"),\n",
        "\n",
        "                          #output layer\n",
        "                          keras.layers.Dense(11, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss = \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpuNuFUsJJS8"
      },
      "source": [
        "#train the network\n",
        "model.fit(inputs_train, targets_train, \n",
        "          validation_data=(inputs_test, targets_test), \n",
        "          batch_size=32,\n",
        "          epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}